---
title: "Exploring Visionâ€Šâ€”â€ŠAutomatic Face Detection and Cropping for Profile Pictures (Swift)"
author: "ZhgChgLi"
date: 2018-10-16T16:01:24.511+0000
last_modified_at: 2024-08-13T08:17:24.185+0000
categories: "ZRealm Dev."
tags: ["swift","machine-learning","facedetection","ios","ios-app-development"]
description: "Practical Application of Vision"
image:
  path: /assets/9a9aa892f9a9/1*c-ioRH_Z2nMYRxSbuBD71A.png
render_with_liquid: false
---

### Exploring Visionâ€Šâ€”â€ŠAutomatic Face Detection and Cropping for Profile Pictures (Swift)

Practical Application of Vision

### \[2024/08/13 Update\]
- Refer to the new article and API: "[iOS Vision framework x WWDC 24 Discover Swift enhancements in the Vision framework Session](../755509180ca8/)"

#### Without further ado, here is a comparison image:

![Before Optimization V.S. After Optimization â€” [Marry Me APP](https://itunes.apple.com/tw/app/%E7%B5%90%E5%A9%9A%E5%90%A7-%E4%B8%8D%E6%89%BE%E6%9C%80%E8%B2%B4-%E5%8F%AA%E6%89%BE%E6%9C%80%E5%B0%8D/id1356057329?ls=1&mt=8){:target="_blank"}](/assets/9a9aa892f9a9/1*c-ioRH_Z2nMYRxSbuBD71A.png)

Before Optimization V.S. After Optimization â€” [Marry Me APP](https://itunes.apple.com/tw/app/%E7%B5%90%E5%A9%9A%E5%90%A7-%E4%B8%8D%E6%89%BE%E6%9C%80%E8%B2%B4-%E5%8F%AA%E6%89%BE%E6%9C%80%E5%B0%8D/id1356057329?ls=1&mt=8){:target="_blank"}

With the recent iOS 12 update, I noticed the new CoreML machine learning framework and found it quite interesting. I began to think about how to incorporate it into our current products.

> **The article on trying out CoreML is now available: [Automatically Predict Article Categories Using Machine Learning, Even Train the Model Yourself](../793bf2cdda0f/)**

CoreML provides the ability to train and reference machine learning models for text and images in an app. Initially, I thought of using CoreML for face recognition to address the issue of cropping heads or faces in the app, as shown on the left in the image above. Faces can easily be cut off due to scaling and cropping if they appear at the edges.

After some online research, I realized my knowledge was limited, and this functionality was already available in iOS 11 through the "Vision" framework, which supports text detection, face detection, image comparison, QR code detection, object tracking, and more.

In this case, I utilized the face detection feature from Vision and optimized it as shown on the right in the image; finding faces and cropping around them.

### Let's get started with the practical implementation:
#### First, let's create a feature that can mark the position of faces and get familiar with how to use Vision.

![Demo APP](/assets/9a9aa892f9a9/1*cpGgpXsBhuiJoZI03WAGUw.png)

Demo APP

As shown in the completed image above, it can mark the positions of faces in the photo.

P.S. It can only mark "faces," not the entire head including hair ðŸ˜…

This program mainly consists of two parts. The first part addresses the issue of white space when resizing the original image to fit into an ImageView. In simple terms, we want the ImageView size to match the image size. Directly inserting the image can cause misalignment as shown below.

![](/assets/9a9aa892f9a9/1*Mb70Ed6pALO-8sllCpb7Qg.png)

You might consider changing the ContentMode to fill, fit, or redraw, but this may cause distortion or cropping of the image.

```swift
let ratio = UIScreen.main.bounds.size.width
// Here, I set the alignment of my UIImageView to 0 on both sides, with an aspect ratio of 1:1

let sourceImage = UIImage(named: "Demo2")?.kf.resize(to: CGSize(width: ratio, height: CGFloat.leastNonzeroMagnitude), for: .aspectFill)
// Using KingFisher's image resizing feature, based on width, with flexible height

imageView.contentMode = .redraw
// Using redraw to fill the contentMode

imageView.image = sourceImage
// Assigning the image

imageViewConstraints.constant = (ratio - (sourceImage?.size.height ?? 0))
imageView.layoutIfNeeded()
imageView.sizeToFit()
// Here, I adjust the constraints of the imageView. For more details, refer to the complete example at the end of the document
```

Here is the translated content:

The above is the processing for images.

_The cropping part uses Kingfisher to assist us, and can also be replaced with other libraries or custom methods._

Next, let's focus on the code directly.
```swift
if #available(iOS 11.0, *) {
    // Supported after iOS 11
    let completionHandle: VNRequestCompletionHandler = { request, error in
        if let faceObservations = request.results as? [VNFaceObservation] {
            // Recognized faces
            
            DispatchQueue.main.async {
                // Operate on UIView, switch back to the main thread
                let size = self.imageView.frame.size
                
                faceObservations.forEach({ (faceObservation) in
                    // Coordinate system conversion
                    let translate = CGAffineTransform.identity.scaledBy(x: size.width, y: size.height)
                    let transform = CGAffineTransform(scaleX: 1, y: -1).translatedBy(x: 0, y: -size.height)
                    let transRect =  faceObservation.boundingBox.applying(translate).applying(transform)
                    
                    let markerView = UIView(frame: transRect)
                    markerView.backgroundColor = UIColor.init(red: 0/255, green: 255/255, blue: 0/255, alpha: 0.3)
                    self.imageView.addSubview(markerView)
                })
            }
        } else {
            print("No faces detected")
        }
    }
    
    // Recognition request
    let baseRequest = VNDetectFaceRectanglesRequest(completionHandler: completionHandle)
    let faceHandle = VNImageRequestHandler(ciImage: ciImage, options: [:])
    DispatchQueue.global().async {
        // Recognition takes time, so it is executed in the background thread to avoid freezing the current screen
        do{
            try faceHandle.perform([baseRequest])
        }catch{
            print("Throws: \(error)")
        }
    }
  
} else {
    //
    print("Not supported")
}
```

The main thing to note is the coordinate system conversion part; the results recognized are in the original coordinates of the image; we need to convert it to the actual coordinates of the ImageView outside to use it correctly.

#### Next, let's focus on today's highlight - cropping the correct position of the avatar according to the position of the face.
```php
let ratio = UIScreen.main.bounds.size.width
// Here, because I set the left and right alignment of my UIImageView to 0, with a ratio of 1:1, details can be found in the complete example at the end

let sourceImage = UIImage(named: "Demo")

imageView.contentMode = .scaleAspectFill
// Use scaleAspectFill mode to fill

imageView.image = sourceImage
// Assign the original image, we will operate on it later

if let image = sourceImage, #available(iOS 11.0, *), let ciImage = CIImage(image: image) {
    let completionHandle: VNRequestCompletionHandler = { request, error in
        if request.results?.count == 1, let faceObservation = request.results?.first as? VNFaceObservation {
            // One face
            let size = CGSize(width: ratio, height: ratio)
            
            let translate = CGAffineTransform.identity.scaledBy(x: size.width, y: size.height)
            let transform = CGAffineTransform(scaleX: 1, y: -1).translatedBy(x: 0, y: -size.height)
            let finalRect =  faceObservation.boundingBox.applying(translate).applying(transform)
            
            let center = CGPoint(x: (finalRect.origin.x + finalRect.width/2 - size.width/2), y: (finalRect.origin.y + finalRect.height/2 - size.height/2))
            // Here is the calculation of the middle point position of the face range
            
            let newImage = image.kf.resize(to: size, for: .aspectFill).kf.crop(to: size, anchorOn: center)
            // Crop the image according to the center point
            
            DispatchQueue.main.async {
                // Operate on UIView, switch back to the main thread
                self.imageView.image = newImage
            }
        } else {
            print("Detected multiple faces or no faces detected")
        }
    }
    let baseRequest = VNDetectFaceRectanglesRequest(completionHandler: completionHandle)
    let faceHandle = VNImageRequestHandler(ciImage: ciImage, options: [:])
    DispatchQueue.global().async {
        do{
            try faceHandle.perform([baseRequest])
        }catch{
            print("Throws: \(error)")
        }
    }
} else {
    print("Not supported")
}
```

The logic is similar to marking the position of a face, the difference is that the avatar part has a fixed size \(e.g. 300x300\), so we skip the first part that requires the Image to fit the ImageView.

Another difference is that we need to calculate the center point of the face area and use this center point as the reference for cropping the image.

![The red dot is the center point of the face area](/assets/9a9aa892f9a9/1*civytcKOguHfVFHYPVWecA.png)

The red dot is the center point of the face area.

#### Final effect image:

![The second before the blink is the original image position](/assets/9a9aa892f9a9/1*WocYjt0xLkqtGVilxfT2LA.gif)

The second before the blink is the original image position.

### Complete app example:

![](/assets/9a9aa892f9a9/1*J8oByw8gBCamIac2TkT1SA.gif)

The code has been uploaded to Github: [Click here](https://github.com/zhgchgli0718/VisionDemo){:target="_blank"}

For any questions or suggestions, feel free to [contact me](https://www.zhgchg.li/contact){:target="_blank"}.

_[Post](https://medium.com/zrealm-ios-dev/vision-%E5%88%9D%E6%8E%A2-app-%E9%A0%AD%E5%83%8F%E4%B8%8A%E5%82%B3-%E8%87%AA%E5%8B%95%E8%AD%98%E5%88%A5%E4%BA%BA%E8%87%89%E8%A3%81%E5%9C%96-swift-9a9aa892f9a9){:target="_blank"} converted from Medium by [ZMediumToMarkdown](https://github.com/ZhgChgLi/ZMediumToMarkdown){:target="_blank"}._
